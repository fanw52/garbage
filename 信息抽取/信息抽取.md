# 信息抽取

## 模型-Transformer
### 位置向量编码
* 位置向量编码两种形式
    * 可学习的位置编码
    * 不可学习的位置编码（正余弦）
        * 正余弦的位置编码通过三角函数和差公式，可以很好地体现相对位置关系
    
* 位置编码采用concat方式
    * 可理解为加上了一个编码后的one hot位置向量
    * https://mp.weixin.qq.com/s/hn4EMcVJuBSjfGxJ_qM3Tw
    

### layer Normalization
* layer normalization沿着channel做归一化,而batch normalization沿着batch做归一化。
因为transformer占用的显存较大，batch size没办法设置地很大，所以使用BN有可能不会起到很好的作用
  
### self-attention

* KQV
    * key和value计算得到attention权重矩阵，并对value进行权重分配得到新的feature

* multi-head
    * 每个head学习的特征侧重点不一样，多head提高模型的表达能力
  

## 序列标注任务
### 主要问题
* 嵌套问题
  * span
* 歧义问题


### 数据增强
  * 伪标签
  * UDA
  * ...
  
### 处理NER任务的思路
* 简单模型（lstm+crf）快速迭代，规则+领域词典进行优化，或者结合分词，句法分析工具等。
* 将重点放在embedding
* span跨度过长，除了规则修正，还可以引入指针网络
* NER任务中的类别不均匀问题，往往从数据采样，loss等角度出发
* 需要尝试一套多层级，多粒度，多策略的解决方案，比如，可以先提取粗粒度的结果，保证召回率，再提取细粒度的结果，保证精准率.

