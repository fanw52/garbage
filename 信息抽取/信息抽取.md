# 信息抽取

## 模型-Transformer
* 简介
  * transformer采用了self-attention机制，抛弃了RNN的顺序结构(顺序计算的过程中信息会丢失)，使得模型可以并行化，并且用有全局信息。
  * attention对输入自适应加权
  * key和query计算特征相似性，并通过softmax转化为概率分布，再将概率分布与value一一相乘获得输出结果
* 输入
  * segment
* 位置向量编码
  * 位置向量编码两种形式
    * 可学习的位置编码
    * 不可学习的位置编码（正余弦）
        * 正余弦的位置编码通过三角函数和差公式，可以很好地体现相对位置关系
    
  * 位置编码采用concat方式
      * 可理解为加上了一个编码后的one hot位置向量
      * https://mp.weixin.qq.com/s/hn4EMcVJuBSjfGxJ_qM3Tw
      

* layer Normalization
  * layer normalization沿着channel做归一化,而batch normalization沿着batch做归一化。
  因为transformer占用的显存较大，batch size没办法设置地很大，所以使用BN有可能不会起到很好的作用
    
* self-attention
  * KQV
    * key和value计算得到attention权重矩阵，并对value进行权重分配得到新的feature
    * 除以根号d_k,假设k和q的每一个维度是独立的，且服从均值为0，方差为1的正态分布，那么kq服从均值为0，方差为d_k的正态分布，除以d_k让方差再变回1

  * multi-head
    * 每个head学习的特征侧重点不一样，多head提高模型的表达能力
  

## 序列标注任务
* 主要问题
  * 嵌套问题
  * 歧义问题


* [数据增强](../数据增强/文本数据增强/文本数据增强.md)

  
* 处理NER任务的思路
  * 简单模型（lstm+crf）快速迭代，规则+领域词典进行优化，或者结合分词，句法分析工具等。
  * 将重点放在embedding
  * span跨度过长，除了规则修正，还可以引入指针网络
  * NER任务中的类别不均匀问题，往往从数据采样，loss等角度出发
  * 需要尝试一套多层级，多粒度，多策略的解决方案，比如，可以先提取粗粒度的结果，保证召回率，再提取细粒度的结果，保证精准率.

* 其他
  * 分类任务计算f1值，三个类别1,2,3，类别为1的数据预测为1，tp+1，预测为2,fn+1，但对于类别2来说fp+1
  