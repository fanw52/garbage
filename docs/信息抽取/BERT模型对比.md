# 各大公司霸榜CLUE的技术总结

## 腾讯【摩天】（模型更小【10亿参数】）
[链接](https://www.jiqizhixin.com/articles/2021-06-24-10)
* 目的：
  开源的预训练模型没有使用搜索领域的语料，导致finetune阶段模型性能不佳
* 基本范式
  * pretrain + finetune + distill

* 数据    
    * 收集了1TB较高质量数据

* 预训练任务
  * masked language model
    * wwm(whole word mask)
      
  * 搜索点击曝光任务（不了解）
    
  * 优化预训练任务和微调任务不一致的问题（预训练阶段的mask token在微调阶段并不出现）
    * BERT中的解决方案：按照一定的比例mask,随机词，保留
    * 【摩天】的解决方案：全随机词，但按照一定的比例替换随机词，近义词等
    
  * 相对位置编码
    * 提出了一种新的位置编码方法
    
  * 训练能力的优化：Pre-LN、混合精度计算、梯度聚集....


  
## 阿里【PLUG】(270亿参数)
中文社区最大的预训练语言模型
[链接](https://hub.baai.ac.cn/view/7715)
* 测试地址：https://nlp.aliyun.com/portal#/BigText_chinese
  
* 数据：1TB+

* 预训练
  * Autoencoding的预训练方式
  * Autoregression的预训练方式
  * 引入 Masked LM 目标来提升 encoder 的表征能力
  * 通过预测文本后半部分来提升 decoder 的生成能力

* 它是在句子级别和词级别两个层次的训练目标中，加强对语言结构信息的建模，从而提高模型的语法学习能力。 这也使得PLUG具有输入文本双向理解能力，能够生成和输入更相关的内容
* 将这个编码器用于生成模型的初始化，并外挂一个6层、8192个隐藏层节点数的解码器，共计训练了100B tokens的训练数据。


## 搜狗
* 从10T数据中筛选出2T预训练语料（数据很大）
* post-LN转变为pre-LN，提升训练效率（不懂原理）
  
* 预训练
  * 引入对比学习训练方法 + cross thought预训练方法
    * 解决原始BERT模型学习出来的cls token向量存在各向异性的问题，大大增强预训练模型的表征能力，使得下游任务效果得到明显提升
  * 加入了根据文章标题生成和段落顺序预测两个任务

## 华为【盘古】（模型更大）
* 数据：40T