Transformer可谓是开启了NLP领域一扇新的大门，直至BERT彻底引爆整个NLP领域，直至现在各种霸榜的结果中，都可以看到它的影子，如此惊人，让人不得不想去了解它。
废话就不多说了，直接进入正题。
BERT，18年谷歌发表，论文采用Transformer结构（17年谷歌发表在论文Attention is all your need上），没错，该结构的核心是Attention机制，但如果光看BERT的结构，再三思量后，可能会觉得也没啥可以让人眼前一亮的东西，那为什么它还会这么火呢？
哈哈，因为它刷新了NLP领域的各种指标呀，嗯嗯嗯，厉害就完事了。让我们思考下，曾经BERT霸榜各种任务背后的原因，没错，BERT在进行各种任务之前，事先用了海量的语料库训练了一个语言模型，后续的各项任务的结果，在此基础上微调得到，可想而知这个预训练语言模型是多么牛。
个人瞥见
对于BERT，个人会从以下几个角度去理解它：(需解释为什么)
1,、Transformer结构
2、输入编码
3、预训练方式
4、下游任务
Transformer结构
Transformer结构的核心是attention机制，对于attention机制，cv领域有一篇文章，个人认为总结的比较好（Non-local Neural Networks），文中将五花八门的attention机制划分成几种形式:Gaussian、Embedded Gaussian、Dot Product、Concatenation。其中，Transformer属于Dot Product类型：将N维的character embedding（中文）映射到M维空间中，并计算各个character之间的相关性，得到TxT的矩阵（T为编码的字符长度），表示各个字符之间的权重关系，有点类似于协方差矩阵，相关性越高，数值越大。来看下Transformer最小的结构单元：

dot product的attention结构
用公式来表示就是：

attention表达式
K,Q,V是上一级输出结果X经过三个不同的线性映射得到的，经MatMul得到TxT的矩阵M，需要注意的是，在MatMul后没有直接链接softmax计算各单词之间的权重关系，而是先通过Scale，对矩阵M进行缩放，目的是为了消除嵌入维度对矩阵M的影响(嵌入维度越高，dot product的结果可能就会越高)，在scale之后，有个可选项Mask，Mask的存在是因为输入单词的长度或长或短，对于那些较短的句子，会用0填充至指定长度，因为0是无效数据，在注意力模块，希望网络尽可能少关注这部分，因此，需要引入Mask，实际操作时，则是乘以一个非常小的数，让softmax后的权重尽可能的接近0.可以看下面的几行代码理解下：
uit = uit * (mask + (1 - mask) * -10000) 
a = tf.nn.softmax(uit, axis=1) 
weighted_input = input_x * a 
其中，mask非0即1，0表示无效的pad数据，1表示表示有效的输入，当mask=0时，uit = -10000*uit；当mask=1时，uit = uit，如此一来后面连接softmax，就可以将pad部分的数据权重降到很小。
为了能够让网络关注输入数据的不同部分，会使用多个上述介绍的模块，每个模块会有一个输出，各输出之间沿着嵌入维度以concat连接，为了能够连接同样的模块，还需要对其进行降维操作，将dim*h维向量映射回dim维向量。

Multi-Head Attention 结构

整个transformer结构就是Multi-Head Attention的叠加。
输入编码
这一部分，一直认为是非常重要的，是transformer结构能够成功的关键，BERT中输入编码包含三个部分：字符嵌入，位置嵌入，句子嵌入。
字符嵌入：随机初始化的K维向量
位置嵌入：BERT中随机初始化，可学习位置编码，但在原始Transformer中，引入cosine，同一位置的位置编码在不同的训练语料中保持不变，从这一点来说，可以表现出绝对位置编码，但我们更关注的是相对位置编码（两个词在一句话中的语义信息，在很多情况下，不会因为绝对位置的改变而改变），对于这一点，也不用担心，可以试着展开cos(i+k)，会意外的建立与cos(i)的线性关系，而线性表达的权重和相对位置k相关，这也就建立了相对位置编码关系。回归到BERT，没有这么复杂的东西，至于这些绝对位置，相对位置关系，都自己去学习吧。
句子嵌入：BERT中，引入了next sentence prediction，作为预训练任务，在“完形填空”的基础上，还需要判断前后两个句子是否存在上下文关联。
预训练方式
BERT是一种自编码模型（自回归是用当前词预测下一个词，自编码是用mask方法遮挡住部分单词，用上下文预测被mask的单词），在训练时，会以一定的概率随机遮挡输入单词，要求输出端能够正确地预测被遮挡的单词。但简单的遮挡存在一定的问题，试想下，预测阶段，会有Mask操作吗？显然是没有的，这就会导致训练与预测输入不一致，为了缓解这个问题，在预训练的过程中，15%的语料需要进行处理，其中的80%进行Mask操作，10%进行词替换操作，剩下的10%保持原样。哎，好像还是存在训练与预测数据分布不一致，那就留给XLNet去解决吧。
实现各种下游任务
Transformer是一种非常强大的特征提取器，相比于LSTM具有强并行性，通过位置编码建立语句中各个单词之间的联系。由于参数数目过于庞大，在实际使用的过程中，只能通过微调的方式，对不同的任务进行建模。比如：
分类任务，我们需要做的就是转换得到输入语料的三种编码方式，然后取输出端[CLS] 位置的向量作为全连接层的输入，进行分类。
命名实体任务，在输出端连接LSTM，FC，CRF，做命名实体识别任务，甚至直接再BERT的输出端，连接FC层，进行softmax操作，进行命名实识别任务，貌似效果也不错。