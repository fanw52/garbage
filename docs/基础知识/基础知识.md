# 基础知识

## 图像基本知识

* 色相（Hue）：颜色的主调
* 饱和度 (Saturability)：每个像素都有个突出的颜色，提高这个突出的颜色（单通道操作），提高饱和度，就是将最突出的通道的值加深
* 亮度 （Value）：每个像素颜色的值（提高亮度是三通道的值都提升）
* 对比度：将每个像素颜色值大于127的提升，小于127的降低（多通道操作）

## CNN相关

* depthwise separable conv?
    * depthwise conv?
        * 一个卷积核只负责一个通道，得到一个通道的结果，而传统卷积是N个卷积核负责N个通道，得到一个通道的结果
    * pointwise conv?
        * 1x1的卷积，depthwise conv无法实现特征的升维与降维，控制模型参数
* 卷积的复杂度和卷积的参数量
    * 传统卷积
        * 复杂度（FLOPs）
            * 卷积一次的计算量
                * 乘法计算量：$ C_{in} * K * K$
                * 加法计算量：$ (C_{in}-1) * K * K + 1$(忽略)
            * 总的计算量（不考虑加法）：$ (C_{in} * K * K) * W * H * C_{out}$​
        * 参数量
            * $K * K * C_{in} * C_{out}$​
    * depthwise separable conv
        * depthwise conv
            * 卷积一次的计算量
                * 乘法计算量：1 * K * K
                * 加法计算量：无
            * 总计算量（不考虑加法）： $K * K * W * H * C_{out}$
        * pointwise conv
            * 卷积一次的计算量
                * 乘法计算量：$C_{in}$
                * 加法计算量：$C_{in}-1$
            * 总计算量（不考虑加法）： $C_{in}*W*H*C_{out}$
        * 总计算量：$K * K * W * H * C_{out} + C_{in} * W * H * C_{out}$​​
        * 深度可分离卷积计算量是传统卷积计算量的：$1/C_{in} + 1/K**2$​​
    * deformable conv
        * 传统卷积的计算方法拥有固定的几何结构，对几何形变的物体建模受限，而dfc通过学习卷积的偏移量，使得卷积核在 input feature map的采样点上发生偏移，实现在当前位置周围随意采样，由于位置的随意性，会出现非整数点的情况， 而这种情况的值，需要通过双线性插值的方法实现。
    * group conv
        * 降低卷积计算的浮点运算量
* 如何理解参数共享？
    * 表面意思：对于input feature map,output feature map的每一个像素点都是由相同权重与input feature map在卷积的操作下得到的
    * 减少参数量
    * 构建局部特征？
* 感受野计算
    * $RF_i = (RF_{i-1}-1)*s+k$
* 卷积的矩阵运算
    * img2col:从左到右，从上到下，取一个kernel范围内的特征，拉平，放置在一行（多通道，水平方向concat）
    * kernel拉平，放置成一列(多通道，垂直方向concat)
    * 矩阵相乘得到相应位置的卷积结果
* [卷积神经网络的反向传播？](https://zhuanlan.zhihu.com/p/81675803)
    * 卷积层
        * 卷积需要先转变成矩阵相乘的形式，那么卷积的反向传播就等于全连接层的反向传播
    * 池化层
        * 最大池化，仅在最大值位置有残差
    * 全连接层
        * 与卷积展开后一致
* 反卷积相比其他其他上采样层的缺点？
  [反卷积与上采样](https://www.zhihu.com/question/328891283/answer/717113611)
    * 反卷积/转置卷积的原理
    * 转置卷积和先上采样在卷积的区别
        * 反卷积设置不当，容易让输出的feature map呈棋盘状。
* 卷积核的大小应该如何选取？
    * 一般来说，卷积和的大小设置为奇数，奇数卷积核的好处：
        * 更容易确定卷积的中心点
        * 奇数更容易padding(保证卷积前后尺寸一致)，padding的幅度设为(k-1)/2时，卷积核设为奇数，padding数目为整数
* 2D卷积和3D卷积的区别，主要存在问题，如何加速计算？
    * 2D卷积相比3D卷积少了一个维度

    * R2D
        * 将视频当做图像处理，将所有的时刻的图片沿着channel维度叠加

    * C3D 3D卷积

    * R(2+1)D
        * 混合卷积方法，用2D卷积（K\*K\*1）+1D卷积(T\*1\*1)替代3D卷积(K\*K\*T)
* 卷积层减少参数的方法？
    * depthwise conv
    * pointwise conv
    * 1x3conv + 3x1conv 替代3x3conv
* **conv上的dropout方式**
    * ？？？？


* 输出feature map的尺寸
    * $(l-k+2p)/s+1$

## dropout的原理

* 随机失活神经元，缓解过拟合的现象（降低神经元之间的依赖关系，防止某些特殊的pattern会激发特定的神经元导致错误的判断），一定程度上起到正则化的作用
* 在训练阶段，输出结果乘以系数1/(1-p)【p是失活概率】【期望E(x)=(1-p)*x】,测试阶段输出结果保持不变【期望E(x)=x，需要手动再乘以概率1-p】， 如此可以保证训练阶段与测试阶段的输出结果的期望一致

## pooling

* max-pooling怎么传递导数？
  ![反向传播图](data/max_pooling反向传播.png)

* 可以看出max pooling只在最大值位置能够传递残差

* cnn里池化的作用？
    * 增加感受野
    * 降低模型的计算量
    * 为网络提供不变性，包括旋转，平移，缩放等

## BN

* 原理
    * 神经网络中，数据在某一层发生偏移，随着层数的增加，这种偏移现象会越发的明显(模型内部激活层特征分布的改变)，进而导致模型优化困难。

    * BN是一种特征归一化技术
        * 调整了数据分布，不考虑激活函数，将每一层的输出特征归一化到均值为0，方差为1的分布，保证了梯度的有效性
        * 可以使用更大的学习率，跳出局部极值
        * $\mu\subset\R^{W*H*C}$​

* 训练
    * 计算每一个batch内的均值与方差迭代并利用滑动平均的方法更新均值方差
        * $X = \beta*X + (1-\beta)*\mu$​​​
        * $V_{100}=0.9V_{99}+0.1\theta_{100}$​
        * $V_{99}=0.9V_{98}+0.1\theta_{99}$​​
        * $V_{98}=0.9V_{97}+0.1\theta_{98}$​​
        * .....
        * 相当于 0.1\*theta_100+ 0.09\*theta_99+...(平均了历史100条数据)

* 测试
    * 方差均值等于训练阶段方差均值的平均
* 缺陷
    * batch size太小均值与方差的计算不稳定
        * 利用beta与gamma将归一化后的特征还原回去
          * `normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity`
          * 归一化sigmoid函数的输入x，会使得特征x,落到非线性特征函数的线性区域，进而会使得模型的表达能力降低，因此，引入了$\gamma$与$\beta$，实现恒等映射，让模型根据数据做出合适的选择。
          * CBA(conv,bn,activate)
    * rnn:不能简单使用BN，因为同一个batch内，数据的有效长度不一致，通过padding的方式，会引入很多无效的信息
        * layer normalization
            * 每个样本维护一个参数，均值与方差都是D-dim的参数
            * $\mu\subset\Re^{D}$
    
* 其他变体
    * 比较：LN和BN都能使用的场景下，BN优于LN，统一特征的归一化的结果更不容易丢失信息。
    * layer normalization:抛弃对batch的依赖，每个样本单独进行normalization，例如，某一层的特征为[B,W,H,C]，那么做layer normalization
      的时候就是基于C\*W\*H求均值、方差（计算当前样本的均值方差，D维）。而batch normalization基于B求W\*H\*C的均值方差（比如有三个通道，分别计算每个通道的均值方差,H*W维）。
    * group normalization:沿着channel方向group，计算每个group内的均值方差，假设输入特征为[N,H,W,C]，那么均值方差的维度为：H*WxC'
    * instance normalization:对某一个特征单独归一化，假设输入特征为[N,H,W,C],那么instance normalization就是在H*W的维度上求均值与方差的过程， 均值与方差的维度为：H\*WxC

## 梯度

* 梯度消失和梯度爆炸引发的原因，有什么方法可以阻止？
    * 神经网络中，梯度是逐层相乘的，随着层数加深
        * 若梯度值小于1，相乘后的梯度愈发接近0，从而产生梯度消失的现象，可通过残差网络，BN等方法解决梯度消失问题
        * 若梯度值大于1，相乘后梯度也会越来越大，可通过梯度截断解决。

## 激活函数

* 作用
    * 增加模型的非线性，提高拟合能力
* Mish
    * $Mish=x * tanh(ln(1+e^x))$
        * 无上边界，避免数值较大进入饱和区
        * 在绝对值较小的负值区域允许一些负值，保证信息更好地流通
        * 平滑的激活函数允许更好的信息流深入网路
            * sigmoid/tanh也是平滑的，但是他们容易进入饱和区
* Swish
    * $swish = x * sigmoid(x)$​
    * 平滑，非单调，无上界，有下界
* Hswish
    * $hswish=x*relu6(x+3)/6$
    * hswish与swish形状上差不多，但是计算量小于hswish，mobilenetv3中使用该函数进行替代

* relu
    * 分段函数
* sigmoid
    * $sigmoid = 1/(1+e^x)$​

## softmax

* 公式 $softmax(x) = e^{x_j}/sum(e^{x_i})$​​
* 怎么防止溢出
    * 分子分母同时除以最大值 $exp(x_j-c)/sum(exp(x_i-c))$
* 作用
    * 将神经元的输出值转变成概率分布的形式，获得每个类别的概率

* log-softmax
    * 计算cross entropy时，可能会出现$y*log(p) = y* log(0)$的情况
    * 为了避免这种情况，采用log-softmax的计算方式需要稍稍改变 $log-softmax = x_i - c - log(sum(exp(x_j-c)))$

## attention

* attention是怎么实现的？
* 利用某种方式计算注意力权重
    * KQV
    * SE
        * 对通道之间的依赖性进行建模，自适应地校准各通道的值

## 参数初始化

* 为什么不能将所有的权重初始化为0？假如都初始化为0会出现什么样的情况？
    * [出现权重对称现象](https://zhuanlan.zhihu.com/p/75879624)
    * https://zhuanlan.zhihu.com/p/364142934

## RNN相关

* RNN为什么会出现梯度消失的现象？
    * 从反向传播的角度，梯度或者是残差通过 连乘 的形式反映，与卷积神经一致，当梯度或残差一直小于0时，那么就会出现梯度消失的现象
* BPTT推导？
    * RNN公式
        * $h_t = sigma(W_t x_t+W_h h_{t-1})$
        * $y_t = sigma(W_o h_t)$
    * 反向传播
        * 先求t时刻的残差与梯度，再求t-1时刻的残差与梯度
* LSTM的公式以及结构是什么？
    * $i_t=\sigma(W_i[h_{t-1};x_t]+b_i)$
    * $f_t=\sigma(W_f[h_{t-1};x_t]+b_f)$
    * $C'_t=tanh(W_c [h_{t-1};x_t]+b_c)$
    * $C_t=i_t \odot C'_t+f_t \odot C_{t-1}$
    * $o_t=\sigma(W_o[h_{t-1};x_t]+b_o)$
    * $h_t=o_t \odot tanh(C_t)$​
    * 输入门控制前一个时刻隐状态的保留，遗忘门控制前一个时刻cell的保留，输出门控制前一个时刻的隐状态有多少能被输出
    * 其中，LSTM中的$h-t$相当于RNN中的$y_t$，$C_t$相当于RNN中的$h_t$

        * 在反向传播的过程中t时刻的残差将会是一个与$f_t$​相关的量，此时可以控制遗忘门的参数使其接近1，那么基于链式法则求得的梯度也不会太小。
* LSTM为什么会防止梯度爆炸?

## CRF

* CRF引入转移矩阵，控制状态之间的转移，建立标签之间的依赖或者约束，需要注意的是，建立标签依赖并不一定需要CRF这样的显式模型，如果模型足够强，CRF甚至是不必要的,甚至是有害的
    * 马尔科夫假设对建立状态之间的关联有一定的限制
    * 维特比解码的时间复杂度较高，训练时间比较长
* 维特比解码
    * 时间复杂度：$O(TN^2)$​
* 相关知识
    * 马尔科夫假设：状态$X_t$只受状态$X_{t-1}$​​的影响，相当于1-gram
    * 马尔科夫过程：每个状态的转移只依赖前n个状态，并且只是一个n阶模型

## 过拟合

* 过拟合的本质？
    * 简单模型，高偏差，低方差（简单模型不容易被噪音干扰）
    * 复杂模型，低偏差，高方差
    * 过拟合：高方差，低偏差
* 过拟合的解决办法？
    * 正则，适当地提高方差，直观的理解就是，更小的权重会使得模型的复杂度降低
    * loss
    * 样本
    * 模型
    * 训练策略，提前终止

## 网络结构

* resnet的结构特点，解决的是什么问题？
    * 随着模型的加深，训练误差增加，测试误差增加【网络退化】，可能因为层数较深，引发了梯度消失/梯度爆炸等问题，阻碍了网络的优化
        * BN层，可以缓解梯度消失或爆炸的问题
        * resnet引入恒等映射结构，提出了残差模块，保证浅层的特征，可以通过恒等映射直接到达深层特征。
        * 反传过程中，保证残差的信息量$1+f(x)$
* resnet的下采样
    * resnet中，卷积层的stride设置为2，替代pooling层（部分保留）
* MobileNet
    * 深度可分离卷积(depthwise separable conv)
        * 卷积核与input feature map对应通道做卷积操作
        * 逐点卷积（pointwise convolution：1x1）
    * 深度可分离卷积+逐点卷积（计算量：$K*K*W*H*C_i+K*K*W*H$）替代普通卷积(计算量$K*K*W*H*C_i*C_o$),计算量约为原先的1/8
* MobileNetV2
    * inverted residuals（升维，卷积，降维），保留更多信息
    * depthwise separable conv
    * pointwise conv

    * common residual(降维，卷积，升维)

* MobileNetV3
    * depthwise separable conv
    * inverted residuals
    * SE模块（squeeze and excitation）
        * squeeze
            * 对$[W,H,C]$的feature map做global avg pooling得到$[1,1,C]$的特征图
        * excitation
            * 对1x1xC的feature map做非线性变换，并结合sigmoid得到每个通道的注意力权重（channel-level），增强重要特征，抑制不重要特征

## transformer

* transformer为什么不能表示位置关系?
    * $q_i k_j = (x_i+p_i) W_q [(x_j+p_j) W_k].T$，假如删除p_i和p_j,那么无论x_i和x_j处于什么位置，其计算结果都相同
* 为什么word embedding、position embedding用add而不是concat?
    * 使用add可以降低输入的维度，减少计算量
    * 存在$W_x$​和$W_y$​使得$(x_i+p_i)* W_y = [x_i;p_i] * W_x$​​成立，因此我们说，使用concat可以等效看作使用add
* transformer里面的MLM(masked language model)有什么作用（传统语言模型：预测下一个词）？
    * 做法：输入一句话，遮住部分单词（使用【mask】替代将要被遮住的词），模型预测被遮住的单词，考虑到在预测阶段，不存在【mask】,这就导致训练与预测出现不一致的现象，因此：
        * 80%概率【mask】替换
        * 10%概率随机替换
        * 10%概率不做替换
* decoder的过程？
    * transformer模型主要包括auto-regression和auto-encoder
        * 自回归：用前一个时刻的预测结果，作为当前时刻的输入，作预测
            * encoder的结果作为key和value，与前一个时刻输出结果（value），先计算注意力权重，再计算特征值
        * 自编码：并行计算一次性获得所有时刻的输出
        * https://congchan.github.io/NLP-attention-03-self-attention/

## BERT

* BERT的输入是什么？有什么改进的地方？
    * 输入
        * Word embedding
        * Position embedding
    * 改进
        * whole word mask
        * Auto-regression转auto-encoder
* BERT、ELMo、wordvec
* bert预训练
    * 

## 项目

* RoI Pooling和RoI Align的区别？
    * 目标：抽取感兴趣区域的特征，并转化为相同大小的feature输出
    * RoI Pooling:将感兴趣区域划分成K*K个区域，再进行池化，如果遇到不能整数划分的情况，向上取整或者向下取整
    * RoI Align：通过双线性插值的方法，解决了边界点是非整数点丢失特征的情况。
    * 双线性插值公式
        * $\varphi(x,y)=\Sigma_{i,j=1}\varphi(x_i,y_i)max(0,1-|x-x_i|)max(0,1-|y-y_i|)$​​
            * $\varphi(x,y)$表示坐标为$(x,y)$的像素插值后的结果
            * $(x_i,y_i)$表示$(x,y)$周围临近的四个坐标
            * 双线性插值的本质：以最近几个像素的距离作为权重，对像素值加权平均，某个像素的权重值等于其对角像素与中心下像素构成的矩阵面积成正比

* OHEM
    * 是一种解决正负样本不均匀的算法
        * 根据目标的score获取得分较高的正样本与负样本作为候选目标，进行模型优化

* ResNet-vd有什么改进之处？
    * resnet-vd针对下采样的模块进行了改进：
        * 将stride=2的1x1卷积替换为 stride=2的avgpool+ stride=1的1x1卷积
            * 保留更多的信息量
        * shortcut部分，将stride=2,1x1的卷积+stride=1,3x3的卷积替换为stride=1,1x1的卷积+stride=2,3x3的卷积
            * 保留更多的信息量

* PPOCR识别部分，相比crnn做了哪些改进？
    * 为了保留更多水平方向的信息，将stride为(2,2)的下采样替换为(2,1)的下采样
    * 为了保留更多垂直方向的信息，将第二个下采样从(2,1)变成(1,1)
    * 最后一层使用(2,2)的max pool，来确保改进前后的尺寸一致

* 损失函数

    * Focal loss
        * $loss= \Sigma_i\alpha(1-p_i)^{\gamma}*y_i*log(p_i)$​​​​​​​​​
        * $(1-p_i)^{\gamma}$​​，$p_i$​越小（困难样本），损失越大
        * $\alpha$是平衡因子
    * Focal loss与OHEM的区别与联系
        * 都是针对困难样本的处理方法
        * Focal loss会直接作用与梯度
        * OHEM是一种样本选择的方法

* 非极大值阈值
    [code](code/nms.md)

* FPN中concate与add有什么区别，从反向传播的角度解释？
    * add操作:$z=[x_i+x_j]W_x$​
    * concat操作：$z=[x_i;x_j]W_y$​
    * 从反向传播的角度来说，个人理解，add与concat一致，因为可以找到一对$W_x;W_y$使得add操作与concat操作结果相等，也可以找到一组参数，使得反传的残差相等

* 语义分割中mIoU计算公式？
    * $mIoU=\Sigma \frac{IoU_i}{N}$​​​
    * $IoU_i=\frac{TP_i}{TP_i+FP_i+FN_i}$​;分母是所有预测为该类别的像素点

* 分类网络想分非常细的类别怎么办？比如区分哈士奇与阿拉斯加？

* 数据集中假如有20%的噪音怎么办？会对模型有什么影响？从损失函数的角度来说明？

* 余弦距离和欧式距离有什么区别？

    * 余弦距离侧重方向的差异，欧式距离侧重距离的绝对差异
    * 根据不用的特征，选择不同的度量距离，比如【登录次数，平均观看时长】，【1，10】和【10，100】，这两组数据余弦距离很近，欧式距离很远，但要是考量用户活跃度时，使用欧式距离更加合适。

* ROC曲线与P-R曲线？

    * 对于不同的数据集，ROC曲线形状基本不变，而P-R曲线形状变化剧烈，因此ROC曲线可以更加客观地衡量模型本身的性能，而P-R曲线可以更加准确地反映在不同数据集上的性能

    * ROC曲线的横坐标为FPR，纵坐标为TPR

        * FPR =FP/N （N个负样本中，被错误预测为正样本的个数），关注负样本
        * TPR =TP/P （P个正样本中，被正确预测为正样本的个数），关注正样本
        * ROC考虑了正样本与负样本，而P-R只考虑正样本
        
    * 如何绘制P-R曲线和ROC曲线？
        * 以不同的阈值，计算得到precision和recall，或者TPR,FPR

* 旋转变换，仿射变换，透视变换？
    * 旋转变换推导？

      * $[x,y]=[\cos\alpha,\sin\alpha]$​​

        * $[x',y']=[\cos(\alpha+\beta),\sin(\alpha+\beta)]$​

        * $$
            [x',y']=[x,y]\begin{bmatrix}
            
            	-cos\alpha & sin\beta\\\\
            	sin\beta & cos\alpha
            \end{bmatrix}
            $$
      
    * 齐次坐标与齐次方程？ https://blog.csdn.net/hty1053240123/article/details/51992398
        * 引入齐次坐标或者齐次方程是为了统一旋转变换，平移变换，缩放变换
          * 分别写出平移变换，旋转变换，缩放变换的矩阵，就可以发现上面的结论
        
    * 透视变换
        * 将二维坐标转化到三维空间，再映射回二维坐标的过程
        * https://blog.csdn.net/flyyufenfei/article/details/80208361
        * https://www.shangmayuan.com/a/a5293b04f1f14748af56b151.html
        * 仿射变换 有6个参数，需要三点确定，透视变换有6个参数，需要四个点确定

* 目标检测中如何确定正样本？

    * 基于anchor的算法中，正负样本划分的界限是：anchor与目标的IoU确定，大于阈值视作是正样本，小于阈值视作是负样本
    * rpn网络通过三组不同的ratio，以及三组不同的scale确定9个anchor，rpn网络的输出，每个位置预测9个目标的偏移量以及score,低于置信度的视作是负样本，高于置信度的视作是正样本。
    * 上述算法，会获取大量的负样本，为了解决正负样本不均匀问题，后续提出了OHEM算法，focal loss算法等
        * OHEM算法，对anchor的置信度排序，取出得分较高的正负样本
        * focal loss算法则是在损失函数上下文章，将anchor预测的置信度引入损失函数，强化困难样本的权重

* SPPNet(spatial pyramid pooling)

    * 是一种特征值resize的方法，考虑到不同目标的感兴趣区域的尺寸各不相同，无法直接连接全连接层做分类回归任务，而SPPNet起到了中间桥梁的作用，将不同尺寸的feature，变化到相同尺寸
        * 是一种适应的池化操作，根据输入feature map的尺寸与输出feature map的尺寸确定pooling的宽度，$kernerl\_size=ceil(input\_feature\_size/output\_feature\_size)$

* FPN(feature pyramid network)

    * FPN的每个分支，连接3x3的卷积预测（4+1+K）维向量，分别表示目标的位置，目标的得分，以及目标的类别
    * 加入使用多个head，那么就需要从3x3的卷积这里入手，开始分裂，每次预测（4+1）维向量，分别表示不同的类别

* yolo中损失函数是怎么计算得到的？

    * 正负样本的mask，去除不符合要求的box
    * 每个位置9个anchor，计算每个anchor与target的IoU，并根据阈值判断正负样本，以及类别 
    * 重点：
        * 假如要设置multi-head，网络部分应该怎么设计？
          * FPN每个位置的输出需要再连接3x3的卷积，预测（4+1+k）维，那么multi-head就是使用多个3x3的卷积，每次预测一个目标。
        * 回归目标应该怎么设计？
          * 原始的yolo中，通过计算anchor与target的重叠面积，表示正负样本，结合mask计算损失，那么在multi-head版本中，需要
            做什么动呢？
            * 这里想到的是，在原始计算的基础上，将box按照类别分别对应到不同的head上？
              * 是否需要再引入一个mask？
                * Multi-head结果concat,得到[w,h,9x(4+1),2]个目标
                * 引入类别mask，掩盖掉不属于当前类别的目
            * yolov5中的计算逻辑
              * Build_target函数处理prediction以及target，确定prediction对应的标签，计算loss
              * https://zhuanlan.zhihu.com/p/183838757

* HSV空间

* ![image-20211214221444438](基础知识.assets/image-20211214221444438.png)

    * 从上面这张图中可以看到，固定Hue，减少Saturation变白，减少Value变黑，Saturation和Value，颜色更加鲜艳，更深

* 上采样

    * 最近邻插值
    * 反卷积

* 损失函数

    * 度量学习损失函数的起源
        * 使用softmax优化损失函数的过程就是最大化softmax分子的过程，实质上就是最大化$W_jf_x$​的过程，其中$f_x$是输入向量，$W_j$是类别权重，最大化的过程就等价于两个向量夹角不断减小的过程，那么我们可以将权重$W_j$视作是$f_x$的类别中心，所以，文字识别中，取全连接层权重$W$做聚类可以获得形近字。
    * center loss
        * 在网络内部引入可学习的类别中心，将类别中心与输入特征的欧式距离作为辅助函数，结合softmax优化
    * amsoftmax
        * 在softmax的基础上做了一些改进，将$W_jf_x$​​​归一化，表示余弦距离的含义，​额外增加margin，控制类间距

* SENet

    * squeeze和excitation，对卷积得到的特征进行squeeze操作，得到channel级的全局特征，然后进行excitation操作，学习channel之间的关系，即attention权重，并乘以原来的特征得到新的特征。简单来说就是学些channel维度的注意力权重，squeeze通常是global average pooling，exciation通常是全连接层【两层】： 降低模型的复杂度，提高泛化性。

    ## Q

    * 负样本怎么做的，存在的问题？
      * w2v的负采样，训练过程中，训练样本是 (A,B)，即输入A，预测B，B是one hot编码后的正样本，负样本很多，怎么采样？按照词频，选取一定的负样本，来更新权重
    * LR使用 MSE损失的问题
      * 通过梯度或者斜率解释
    * 最长无重复字母的子串
    * 一数组大致有序，每个数与其排序后的位置索引最多差k，排序该数组
      * 插入排序
      * 堆排序
        * 堆：完全二叉树，子节点不小于或者不大于父节点
          * 建堆，得到一个最小堆或者最大堆
          * 假设是最小堆，每次将堆定元素与最后一个元素交换，交换后，堆的性质就不满足了，需要进行堆化，以维护堆的性质。
          * 堆排序的时间复杂度是 O(nlogn)，建堆的时间复杂度是O(n)，空间复杂度是O(1)
          * 伪代码
            * 核心：堆化
              * 给定第i个节点node，以及堆的大小，获取node节点的左右节点，判断左右节点是否比node节点大，如果大则调整位置，调整位置后，会打破子节点的堆的性质，因此需要递归的调用
            * 建堆
              * 从中心位置开始，依次向前堆化，因为最后一层在堆化过程中必然满堆的性质
            * 堆排序
              * 依次将堆顶元素与最后一个位置的元素交换，然后堆化
    * 召回排序，负样本的选择及其区别？
    * 正负样本比例变化，AUC的波动？AUC实际衡量的是什么？
    * xgb
      * Loss 引入了二阶导数信息
      * loss中引入将叶子节点数目当做正则项
    * svm
      * 对于线性可分的情况，就是求解每个类别样本最远的超平面
      * 线性不可分的情况，引入核函数将每个样本隐式的映射到高维空间，可分的超平面
    * bert
    * Lgb
    * paddleocr中采用resnet-vd？
      * 用1x1conv;s=1 + 2x2 agv pooling;s=2替代 1x1 conv;s=2,解决了部分特征信息丢失的问题

