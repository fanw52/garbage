# CenterLoss
* 主要贡献
  * 提出了可学习类别中心，最小化feature map与类别中心的欧式距离作为损失函数值
  * 
    
* 核心思想
    * softmax损失函数的分母 Wx, 其中W是随机初始化的权重，x为feature map,
    * 对比余弦距离与softmax损失函数，可以发现，余弦距离与softmax损失函数的形式是一致的
    * 余弦距离的分子：Wx（余弦距离可以表示两个向量的相似度）
    * softmax函数的分子： exp(Wx)
    * softmax函数计算cross entropy的过程实际等价于让余弦距离逼近1的过程，从这个角度来说，当损失值越来越小的时候，
        权重W就越来越逼近样本的中心。那么模型最后一层的权重实际上可以看做是样本类别中心
      
    * 基于上述假设，作者引入了可学习的类别中心。